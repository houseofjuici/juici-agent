// Serverless function for chat API
export default async function handler(req, res) {
  try {
    // Set CORS headers
    res.setHeader('Access-Control-Allow-Origin', '*');
    res.setHeader('Access-Control-Allow-Methods', 'POST, OPTIONS');
    res.setHeader('Access-Control-Allow-Headers', 'Content-Type');
    
    if (req.method === 'OPTIONS') {
      return res.status(200).end();
    }
    
    if (req.method !== 'POST') {
      return res.status(405).json({ error: 'Method not allowed' });
    }
    
    const { agent_name, message } = req.body;
    
    if (!agent_name || !message) {
      return res.status(400).json({ error: 'Missing required parameters' });
    }
    
    // In a Vercel environment, we'll send a mock response
    // In production, you'd integrate with an actual LLM service
    const response = {
      role: 'assistant',
      content: `This is a mock response from ${agent_name} for the message: "${message.substring(0, 50)}${message.length > 50 ? '...' : ''}". In a production environment, this would be generated by an AI model.`,
      id: Date.now()
    };
    
    // Simulate a delay to mimic API processing
    await new Promise(resolve => setTimeout(resolve, 1000));
    
    return res.status(200).json(response);
  } catch (error) {
    console.error('Error in chat API:', error);
    return res.status(500).json({ error: 'Internal server error' });
  }
} 